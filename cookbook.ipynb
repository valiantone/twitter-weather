{
 "metadata": {
  "name": "",
  "signature": "sha256:0e942f1379056f4fd484bcb4950b78aa1e637404989ada3134b9c2cae1985181"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Appendix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Code for initializing the tf-idf score for all the words in our tweet\n",
      "data set'''\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import SVR, LinearSVC\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
      "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk import word_tokenize\n",
      "\n",
      "class TweetTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self,doc):\n",
      "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
      "\n",
      "train_data = pd.read_csv(open('train.csv'),quotechar='\"')\n",
      "raw_tweets = np.array(train_data['tweet'][:10000])\n",
      "\n",
      "pred = train_data[:10000].copy()\n",
      "varcs = train_data.columns[13:].tolist()\n",
      "vectorizer = TfidfVectorizer (min_df=10,tokenizer=TweetTokenizer () )\n",
      "vectorizer.fit(raw_tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "TfidfVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=10,\n",
        "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
        "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
        "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "        tokenizer=<__main__.TweetTokenizer object at 0x11A4AE70>,\n",
        "        use_idf=True, vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This next segment consists of training the individual classifeirs based on tf-idf features\n",
      "num_folds = 5\n",
      "k_fold = KFold(10000,n_folds=num_folds)\n",
      "\n",
      "for train_indices,test_indices in k_fold:\n",
      "    train_raw = raw_tweets[train_indices].tolist()\n",
      "    test_raw  = raw_tweets[test_indices].tolist()\n",
      "    X_train = vectorizer.transform(train_raw)\n",
      "    X_test  = vectorizer.transform(test_raw)\n",
      "    for variable in varcs:\n",
      "        y_train = np.array(train_data[variable])[train_indices]\n",
      "        clf = SVR(verbose=True)\n",
      "        clf.fit(X_train,y_train)\n",
      "        pred[variable][test_indices] = clf.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM][LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibSVM]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import mean_absolute_error,mean_squared_error,\\\n",
      "...r2_score, explained_variance_score\n",
      "for var in varcs:\n",
      "    print var,(mean_squared_error(np.array(train_data[var][:10000]) , np.array(pred[var])) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "k1 0.0209699299211\n",
        "k2 0.0721354739923\n",
        "k3 0.0117528542131\n",
        "k4 0.076395363891\n",
        "k5 0.0243116079052\n",
        "k6 0.0108051875076\n",
        "k7 0.157690571638\n",
        "k8 0.0111981066979\n",
        "k9 0.025505039276\n",
        "k10 0.0791642423279\n",
        "k11 0.0355855487799\n",
        "k12 0.108484230084\n",
        "k13 0.10243987449\n",
        "k14 0.0213626484225\n",
        "k15 0.0371992508491\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_folds = 5\n",
      "k_fold = KFold(10000,n_folds=num_folds)\n",
      "\n",
      "for train_indices,test_indices in k_fold:\n",
      "    train_raw = raw_tweets[train_indices].tolist()\n",
      "    test_raw  = raw_tweets[test_indices].tolist()\n",
      "    X_train = vectorizer.transform(train_raw)\n",
      "    X_test  = vectorizer.transform(test_raw)\n",
      "    for variable in varcs:\n",
      "        y_train = np.array(train_data[variable])[train_indices]\n",
      "        clf = LinearSVC(verbose=True)\n",
      "        clf.fit(X_train,y_train)\n",
      "        pred[variable][test_indices] = clf.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear][LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[LibLinear]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import mean_absolute_error,mean_squared_error,\\\n",
      "...r2_score, explained_variance_score\n",
      "for var in varcs:\n",
      "    print var,(mean_squared_error(np.array(train_data[var][:10000]) , np.array(pred[var])) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "k1 0.0097908669\n",
        "k2 0.0325012663\n",
        "k3 0.0031688083\n",
        "k4 0.0424182573\n",
        "k5 0.009149085\n",
        "k6 0.0009371159\n",
        "k7 0.0962323411\n",
        "k8 0.0017074155\n",
        "k9 0.0293632068\n",
        "k10 0.0212666662\n",
        "k11 0.0094726752\n",
        "k12 0.0245770262\n",
        "k13 0.0301686895\n",
        "k14 0.0049818238\n",
        "k15 0.0127257934\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for train_indices,test_indices in k_fold:\n",
      "    train_raw = raw_tweets[train_indices].tolist()\n",
      "    test_raw  = raw_tweets[test_indices].tolist()\n",
      "    X_train = vectorizer.transform(train_raw)\n",
      "    X_test  = vectorizer.transform(test_raw)\n",
      "    for variable in varcs:\n",
      "        y_train = np.array(train_data[variable])[train_indices]\n",
      "        clf = LinearRegression()\n",
      "        clf.fit(X_train,y_train)\n",
      "        pred[variable][test_indices] = clf.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import mean_absolute_error,mean_squared_error,\\\n",
      "...r2_score, explained_variance_score\n",
      "for var in varcs:\n",
      "    print var,(mean_squared_error(np.array(train_data[var][:10000]) , np.array(pred[var])) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "k1 0.00602043807843\n",
        "k2 0.0297331732238\n",
        "k3 0.00281033100276\n",
        "k4 0.0332529150941\n",
        "k5 0.006072105698\n",
        "k6 0.000512326059307\n",
        "k7 0.0791000071524\n",
        "k8 0.00145372289674\n",
        "k9 0.0244149497234\n",
        "k10 0.0247907943488\n",
        "k11 0.00980405700188\n",
        "k12 0.0296912909398\n",
        "k13 0.032593782697\n",
        "k14 0.00497234325194\n",
        "k15 0.00974251616894\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for train_indices,test_indices in k_fold:\n",
      "    train_raw = raw_tweets[train_indices].tolist()\n",
      "    test_raw  = raw_tweets[test_indices].tolist()\n",
      "    X_train = vectorizer.transform(train_raw)\n",
      "    X_test  = vectorizer.transform(test_raw)\n",
      "    for variable in varcs:\n",
      "        y_train = np.array(train_data[variable])[train_indices]\n",
      "        clf = GaussianNB()\n",
      "        clf.fit(X_train.toarray(),y_train)\n",
      "        pred[variable][test_indices] = clf.predict(X_test.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import mean_absolute_error,mean_squared_error,\\\n",
      "...r2_score, explained_variance_score\n",
      "for var in varcs:\n",
      "    print var,(mean_squared_error(np.array(train_data[var][:10000]) , np.array(pred[var])) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "k1 0.0421154739\n",
        "k2 0.3147765216\n",
        "k3 0.0058811776\n",
        "k4 0.2717243936\n",
        "k5 0.0757058565\n",
        "k6 0.0058239565\n",
        "k7 0.3468708198\n",
        "k8 0.0040071223\n",
        "k9 0.0435699361\n",
        "k10 0.4252620508\n",
        "k11 0.1754831974\n",
        "k12 0.4127103913\n",
        "k13 0.3606009242\n",
        "k14 0.0621027042\n",
        "k15 0.1755857626\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for train_indices,test_indices in k_fold:\n",
      "    train_raw = raw_tweets[train_indices].tolist()\n",
      "    test_raw  = raw_tweets[test_indices].tolist()\n",
      "    X_train = vectorizer.transform(train_raw)\n",
      "    X_test  = vectorizer.transform(test_raw)\n",
      "    for variable in varcs:\n",
      "        y_train = np.array(train_data[variable])[train_indices]\n",
      "        clf = Lasso(alpha = 100.0)\n",
      "        clf.fit(X_train,y_train)\n",
      "        pred[variable][test_indices] = clf.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import mean_absolute_error,mean_squared_error,\\\n",
      "...r2_score, explained_variance_score\n",
      "for var in varcs:\n",
      "    print var,(mean_squared_error(np.array(train_data[var][:10000]) , np.array(pred[var])) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "k1 0.015590926472\n",
        "k2 0.0762601380716\n",
        "k3 0.00322431888149\n",
        "k4 0.0820833417555\n",
        "k5 0.023768497226\n",
        "k6 0.00109766323204\n",
        "k7 0.148480917148\n",
        "k8 0.00194111316969\n",
        "k9 0.0250199917984\n",
        "k10 0.0839172863957\n",
        "k11 0.0330397879666\n",
        "k12 0.118983947602\n",
        "k13 0.108719238092\n",
        "k14 0.0149959108169\n",
        "k15 0.0393244422266\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for train_indices,test_indices in k_fold:\n",
      "    train_raw = raw_tweets[train_indices].tolist()\n",
      "    test_raw  = raw_tweets[test_indices].tolist()\n",
      "    X_train = vectorizer.transform(train_raw)\n",
      "    X_test  = vectorizer.transform(test_raw)\n",
      "    for variable in varcs:\n",
      "        y_train = np.array(train_data[variable])[train_indices]\n",
      "        clf = Ridge()\n",
      "        clf.fit(X_train,y_train)\n",
      "        pred[variable][test_indices] = clf.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import mean_absolute_error,mean_squared_error,\\\n",
      "...r2_score, explained_variance_score\n",
      "for var in varcs:\n",
      "    print var,(mean_squared_error(np.array(train_data[var][:10000]) , np.array(pred[var])) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "k1 0.00584969069627\n",
        "k2 0.027300469799\n",
        "k3 0.00251083483011\n",
        "k4 0.0303939925568\n",
        "k5 0.00589924381182\n",
        "k6 0.000531689261967\n",
        "k7 0.0714277073185\n",
        "k8 0.00141073713812\n",
        "k9 0.0220738689999\n",
        "k10 0.0228558501466\n",
        "k11 0.00919053530624\n",
        "k12 0.0273036425235\n",
        "k13 0.0298476674559\n",
        "k14 0.00443481775848\n",
        "k15 0.00932317478217\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Naives Bayes Classifier using POS tags as features\n",
      "import re\n",
      "import random\n",
      "import nltk\n",
      "import csv\n",
      "from nltk.stem.lancaster import LancasterStemmer\n",
      "from nltk.classify.api import MultiClassifierI\n",
      "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
      "\n",
      "tokenizer = nltk.TreebankWordTokenizer()\n",
      "lst=[]\n",
      "reader=[]\n",
      "testFeatures=[]\n",
      "test=[]\n",
      "limit=0\n",
      "mean=0\n",
      "#predictedLabels=[]\n",
      "\n",
      "def con_uni(text):\n",
      "    text =\"\".join(text)\n",
      "    text = text.lower()\n",
      "    #text = re.sub(\"\\d+\", \"NUM\", text) # convert numerics to NUM\n",
      "    text = re.sub(\"[^a-zA-Z]\", \" \", text) # remove non {alph, !, ?, %}\n",
      "    # do more preprocessing if necessary\n",
      "    unitk = tokenizer.tokenize(text)\n",
      "    #unitk = \" \".join(unitk)\n",
      "    #st = LancasterStemmer()\n",
      "    #st.stem(unitk)\n",
      "    return(unitk)\n",
      "\n",
      "def word_feats(words):\n",
      "    return dict([('keyword(%s)' %word[1], True)for word in  nltk.pos_tag(words)])\n",
      "\n",
      "def maketext():\n",
      "\n",
      "    with open('train.csv', 'rb') as fil:\n",
      "        global reader, test\n",
      "        reader = csv.reader(fil,delimiter=',')\n",
      "        i=0\n",
      "        tup = []\n",
      "        global limit\n",
      "        limit=60000\n",
      "        for row in reader:\n",
      "            Tweet = con_uni(row[1])\n",
      "            Tweet = word_feats(Tweet)           \n",
      "            if row[13]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k1'))\n",
      "            if row[14]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k2'))\n",
      "            if row[15]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k3'))\n",
      "            if row[16]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k4'))\n",
      "            if row[17]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k5'))\n",
      "            if row[18]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k6'))\n",
      "            if row[19]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k7'))\n",
      "            if row[20]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k8'))\n",
      "            if row[21]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k9'))\n",
      "            if row[22]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k10'))\n",
      "            if row[23]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k11'))\n",
      "            if row[24]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k12'))\n",
      "            if row[25]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k13'))\n",
      "            if row[26]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k14'))\n",
      "            if row[27]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k15'))\n",
      " \n",
      "            if i>=limit:\n",
      "                test.append(row)\n",
      "                testFeatures.append(Tweet)\n",
      "            i=i+1\n",
      "            \n",
      "            \n",
      "\n",
      "if __name__ ==\"__main__\":\n",
      "    maketext()\n",
      "    classifier = nltk.NaiveBayesClassifier.train(lst)\n",
      "    id=1\n",
      "    pl=[]\n",
      "    for t in testFeatures:\n",
      "        predictedLabels=classifier.prob_classify(t)\n",
      "        blue=[]\n",
      "        for n in range(16):\n",
      "            blue.append(predictedLabels.prob(\"k\"+str(n)))\n",
      "        #print id, blue\n",
      "        pl.append(blue)\n",
      "        id+=1\n",
      "    j=0\n",
      "    mse=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
      "    for i in test:\n",
      "        #print i, pl[j]\n",
      "        for k in range(13,28):\n",
      "            #print i[k]\n",
      "            x=float(i[k])\n",
      "            y=float(pl[j][k-13])\n",
      "            temp=pow(x-y,2)\n",
      "            mse[k-13]+=temp\n",
      "        j+=1\n",
      "    for l in range(0,15):\n",
      "        mse[l]=pow(mse[l]/j,0.5)\n",
      "        mean=mean+mse[l]\n",
      "        print \"k\"+str(l+1),mse[l]\n",
      "    mean=mean/15\n",
      "    print \"Average:\"\n",
      "    print mean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "k1 0.124337528641\n",
        "k2 0.280650901077\n",
        "k3 0.117991553736\n",
        "k4 0.314391724724\n",
        "k5 0.183534798439\n",
        "k6 0.0365617173022\n",
        "k7 0.472487795905\n",
        "k8 0.185779371851\n",
        "k9 0.168647924072\n",
        "k10 0.288482284697\n",
        "k11 0.205264715941\n",
        "k12 0.356694778992\n",
        "k13 0.336472070652\n",
        "k14 0.179082658355\n",
        "k15 0.201649577369\n",
        "Average:\n",
        "0.23013529345\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#increasing context by using bigram-features and training a Naives Bayes Classifier\n",
      "tokenizer = nltk.TreebankWordTokenizer()\n",
      "lst=[]\n",
      "reader=[]\n",
      "testFeatures=[]\n",
      "test=[]\n",
      "limit=0\n",
      "mean=0\n",
      "#predictedLabels=[]\n",
      "\n",
      "def con_bi(text):\n",
      "    text =\"\".join(text)\n",
      "    text = text.lower()\n",
      "    #text = re.sub(\"\\d+\", \"NUM\", text) # convert numerics to NUM\n",
      "    text = re.sub(\"[^a-zA-Z]\", \" \", text) # remove non {alph, !, ?, %}\n",
      "    # do more preprocessing if necessary\n",
      "    lines = text.splitlines()\n",
      "    for line in lines:\n",
      "            unitk = tokenizer.tokenize(line)\n",
      "            unitk1 = \" \".join(unitk)\n",
      "            BG=\"\"\n",
      "            for pair in nltk.bigrams(unitk):\n",
      "                bigr = \"_\".join(pair)\n",
      "                BG = BG +\" \"+ bigr\n",
      "            return(BG)\n",
      "        \n",
      "\n",
      "def word_feats(words):\n",
      "    return dict([('keyword(%s)' %word, True)for word in words])\n",
      "\n",
      "def maketext():\n",
      "\n",
      "    with open('train.csv', 'rb') as fil:\n",
      "        global reader, test\n",
      "        reader = csv.reader(fil,delimiter=',')\n",
      "        i=0\n",
      "        tup = []\n",
      "        global limit\n",
      "        limit=60000\n",
      "        for row in reader:\n",
      "            Tweet = con_bi(row[1])\n",
      "            Tweet = word_feats(Tweet)           \n",
      "            if row[13]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k1'))\n",
      "            if row[14]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k2'))\n",
      "            if row[15]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k3'))\n",
      "            if row[16]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k4'))\n",
      "            if row[17]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k5'))\n",
      "            if row[18]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k6'))\n",
      "            if row[19]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k7'))\n",
      "            if row[20]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k8'))\n",
      "            if row[21]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k9'))\n",
      "            if row[22]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k10'))\n",
      "            if row[23]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k11'))\n",
      "            if row[24]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k12'))\n",
      "            if row[25]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k13'))\n",
      "            if row[26]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k14'))\n",
      "            if row[27]!=\"0\":\n",
      "                if (i<limit):\n",
      "                    lst.append((Tweet,'k15'))\n",
      " \n",
      "            if i>=limit:\n",
      "                test.append(row)\n",
      "                testFeatures.append(Tweet)\n",
      "            i=i+1\n",
      "            \n",
      "            \n",
      "            \n",
      "\n",
      "if __name__ ==\"__main__\":\n",
      "    maketext()\n",
      "    classifier = nltk.NaiveBayesClassifier.train(lst)\n",
      "    id=1\n",
      "    pl=[]\n",
      "    for t in testFeatures:\n",
      "        predictedLabels=classifier.prob_classify(t)\n",
      "        blue=[]\n",
      "        for n in range(16):\n",
      "            blue.append(predictedLabels.prob(\"k\"+str(n)))\n",
      "        #print id, blue\n",
      "        pl.append(blue)\n",
      "        id+=1\n",
      "    j=0\n",
      "    mse=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
      "    for i in test:\n",
      "        #print i, pl[j]\n",
      "        for k in range(13,28):\n",
      "            #print i[k]\n",
      "            x=float(i[k])\n",
      "            y=float(pl[j][k-13])\n",
      "            temp=pow(x-y,2)\n",
      "            mse[k-13]+=temp\n",
      "        j+=1\n",
      "    for l in range(0,15):\n",
      "        mse[l]=pow(mse[l]/j,0.5)\n",
      "        mean=mean+mse[l]\n",
      "        print \"k\"+str(l+1),mse[l]\n",
      "    mean=mean/15\n",
      "    print \n",
      "    print \"Average\"\n",
      "    print mean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "k1 0.124337528641\n",
        "k2 0.278596007904\n",
        "k3 0.0833506096314\n",
        "k4 0.31200887281\n",
        "k5 0.16343232131\n",
        "k6 0.0454368151776\n",
        "k7 0.471505427356\n",
        "k8 0.188554200313\n",
        "k9 0.168511111417\n",
        "k10 0.288316229946\n",
        "k11 0.193939967618\n",
        "k12 0.361168010841\n",
        "k13 0.33289100384\n",
        "k14 0.150033801079\n",
        "k15 0.20192176611\n",
        "\n",
        "Average\n",
        "0.2242669116\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "End of Appendix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}